{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Customer Churn\n",
    "\n",
    "1. **Business Understanding** â€“ High customer churn rate. How can we identify and reduce and/or eliminate churn\n",
    "2. **Data Understanding** â€“ Explore distributions, what columns will contribute to model prediction\n",
    "3. **Data Preparation** â€“ Clean, encode, scale, and split the data.\n",
    "4. **Modeling** â€“ Train baseline and tuned classification models.\n",
    "5. **Evaluation** â€“ Assess models using standard classification metrics.\n",
    "6. **Deployment / Recommendations** â€“ Translate findings into business actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Pipeline\n",
    "\n",
    "This notebook applies supervised machine learning to predict telecom customer churn â€” identifying which customers are most likely to leave.  \n",
    "We use two models: Logistic Regression (baseline) and a Decision Tree (tuned for optimal performance).  \n",
    "The goal is to improve retention strategies by identifying key churn factors and generating actionable business insights. \n",
    "We start by importing data structures and manipulation library, numerical computation and machine learning modules. Also visualization libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_theme(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "\n",
    "Load the telecom churn dataset into a pandas DataFrame, preview its structure, and inspect data types.  \n",
    "This helps confirm the target variable and overall data quality before cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bigml_59c28831336c6604c800002a.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-24428df27db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Start by checking the columns, data types, and target variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigml_59c28831336c6604c800002a.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Overview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bigml_59c28831336c6604c800002a.csv'"
     ]
    }
   ],
   "source": [
    "# Understanding the dataset structure\n",
    "# Start by checking the columns, data types, and target variable\n",
    "\n",
    "df = pd.read_csv(\"bigml_59c28831336c6604c800002a.csv\")\n",
    "\n",
    "# Overview\n",
    "#print(\"Shape:\", df.shape)\n",
    "display(df.head(3))\n",
    "#print(\"\\nInfo:\")\n",
    "#print(df.info())\n",
    "#print(\"\\nMissing values per column:\")\n",
    "#display(df.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Column Names\n",
    "\n",
    "Normalize and clean column names by removing spaces, punctuation, and inconsistencies.  \n",
    "This ensures column names are standardized for smooth downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up column names\n",
    "# Convert spaces and punctuation to underscores for consistency\n",
    "df.columns = (\n",
    "    df.columns.str.strip()\n",
    "              .str.replace(\" \", \"_\")\n",
    "              .str.replace(\"?\", \"\")\n",
    "              .str.replace(\"/\", \"_\")\n",
    "              .str.replace(\"-\", \"_\")\n",
    "              .str.lower()\n",
    ")\n",
    "# Verify cleaned names\n",
    "#df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Column Check and Encoding\n",
    "\n",
    "Inspect the target column (`churn`) to confirm valid boolean entries (True/False).  \n",
    "Convert them to numeric (1 for churn, 0 for non-churn) for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution\n",
    "# Confirm, inspect and encode the churn variable (target)\n",
    "\n",
    "# Confirm unique values\n",
    "print(\"Unique values in churn column:\", df[\"churn\"].unique())\n",
    "\n",
    "# Convert boolean True/False â†’ 1/0\n",
    "df[\"churn\"] = df[\"churn\"].astype(int)\n",
    "\n",
    "# Verify conversion\n",
    "print(\"\\nEncoded churn distribution:\")\n",
    "display(\n",
    "    df[\"churn\"].value_counts().to_frame(\"count\").assign(\n",
    "        pct=lambda t: (t[\"count\"] / t[\"count\"].sum()).round(3)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Verified Unique values in churn column (int):\", df[\"churn\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Features and Target\n",
    "\n",
    "Split the dataset into:\n",
    "- **Features (X):** all customer attributes.\n",
    "- **Target (y):** the churn label (1 = churn, 0 = no churn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features/independent variable and target/dependent variable\n",
    "\n",
    "X = df.drop(columns=[\"churn\"])\n",
    "y = df[\"churn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Feature Types\n",
    "\n",
    "Determine which columns are numeric and which are categorical.  \n",
    "This guides preprocessing â€” scaling for numeric data, encoding for categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical vs numeric features\n",
    "# automatically detect which columns are categorical vs numeric\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"\\nCategorical columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data\n",
    "\n",
    "Check for missing values in the dataset.  \n",
    "If any are found, drop those rows to maintain clean input for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking if there are any missing values\n",
    "# Remove records with missing data \n",
    "\n",
    "print(\"Missing values:\")\n",
    "print(df.isna().sum().sum())\n",
    "\n",
    "# Drop if needed\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train and Test Sets\n",
    "\n",
    "Split data into 70% training and 30% testing sets using stratification to preserve churn balance.  \n",
    "This prepares the data for unbiased model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test sets\n",
    "# stratify to maintain churn balance (we what the same amount of churn in train and test data sets)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train_raw.shape, X_test_raw.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Setup\n",
    "\n",
    "Define preprocessing pipelines:\n",
    "- **StandardScaler:** normalizes numeric features.\n",
    "- **OneHotEncoder:** encodes categorical variables.\n",
    "This ensures consistent, model-ready data transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (encode categorical, scale numeric)\n",
    "# Will use OneHotEncoder for categorical features\n",
    "# Scaling data makes the largets number and smallest number become a percentage that is proportionate to each other\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Class Balance and Summary Statistics\n",
    "\n",
    "Check that the churn ratio remains consistent in both training and test sets.  \n",
    "Review summary statistics to confirm feature integrity before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying class balance and basics stats\n",
    "# checking before modeling\n",
    "\n",
    "print(\"Churn ratio in train set:\")\n",
    "display(y_train.value_counts(normalize=True).round(3))\n",
    "\n",
    "print(\"Churn ratio in test set:\")\n",
    "display(y_test.value_counts(normalize=True).round(3))\n",
    "\n",
    "# Quick numeric summary\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Baseline Model)\n",
    "\n",
    "Train an interpretable Logistic Regression model as the baseline.  \n",
    "Evaluate performance with Accuracy, Precision, Recall, F1 Score, and ROC-AUC metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Baseline Logistic Regression\n",
    "# =============================================\n",
    "\"\"\"\n",
    "this first model serves as the baseline. Logistic Regression is an interpretable model that predicts the probability\n",
    "of churn(1) vs non-churn(0). It helps to establish a performance benchmark before moving to more complex tree models\n",
    "\"\"\" \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, ConfusionMatrixDisplay, roc_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "# Create pipeline: preporcessing + model(logistic Regression)\n",
    "\n",
    "logreg_pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model on training data\n",
    "logreg_pipeline.fit(X_train_raw, y_train)\n",
    "\n",
    "# Predict churn on test data\n",
    "y_pred_lr = logreg_pipeline.predict(X_test_raw)\n",
    "\n",
    "# Predict churn probabilities (used for ROC curve)\n",
    "y_prob_lr = logreg_pipeline.predict_proba(X_test_raw)[:,1]\n",
    "\n",
    "# Verify shape alignment\n",
    "#print(\"Length of y_test:\", len(y_test))\n",
    "#print(\"Length of y_pred_lr\", len(y_pred_lr))\n",
    "\n",
    "# Evaluate the Model\n",
    "print(\"== Logistic Regression Performance ==\\n\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "roc_auc_lr = roc_auc_score(y_test, y_prob_lr)\n",
    "print(f\"ROC-AUC: {roc_auc_lr:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(logreg_pipeline, X_test_raw, y_test, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_lr)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"Logistic Regression (AUC = {roc_auc_lr:.2f})\", color='blue')\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree (Baseline Model)\n",
    "\n",
    "Train and evaluate a Decision Tree classifier using the same preprocessing pipeline.  \n",
    "Visualize results with a Confusion Matrix and ROC Curve for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Decision Tree Baseline Model\n",
    "# ============================================\n",
    "\n",
    "\"\"\"\n",
    "In this step, we train and evaluate a simple Decision Tree Classifier using\n",
    "the same preprocessing pipeline (scaling + encoding). This model introduces \n",
    "non-linearity and interpretable splits, allowing us to compare performance \n",
    "against the logistic regression baseline.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    ConfusionMatrixDisplay, roc_curve\n",
    ")\n",
    "\n",
    "# Create pipeline: preprocessor + model(decision tree)\n",
    "dt_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "dt_pipeline.fit(X_train_raw, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt_pipeline.predict(X_test_raw)\n",
    "y_prob_dt = dt_pipeline.predict_proba(X_test_raw)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"== Decision Tree (Baseline) Performance ==\\n\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_dt):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_dt):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_dt):.3f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_dt):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_prob_dt):.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(dt_pipeline, X_test_raw, y_test, cmap=\"Greens\")\n",
    "plt.title(\"Confusion Matrix - Decision Tree (Baseline)\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_dt)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"Decision Tree (AUC = {roc_auc_score(y_test, y_prob_dt):.2f})\", color='green')\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Decision Tree (Baseline)\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Hyperparameter Tuning\n",
    "\n",
    "Use GridSearchCV to optimize the Decision Treeâ€™s hyperparameters.  \n",
    "This prevents overfitting and improves model generalization to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Decision Tree Hyperparameter Tuning\n",
    "# ============================================\n",
    "\n",
    "\"\"\"\n",
    "In this step, we perform hyperparameter tuning using GridSearchCV to optimize \n",
    "Decision Tree depth and split criteria. This helps prevent overfitting and \n",
    "improves the modelâ€™s ability to generalize to unseen customer data.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    \"model__max_depth\": [3, 5, 7, 10, None],\n",
    "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4, 10],\n",
    "    \"model__criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "# Pipeline: reuse preprocessor + DecisionTreeClassifier\n",
    "dt_tuned = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_tuned,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_raw, y_train)\n",
    "\n",
    "print(\"== Grid Search Complete ==\")\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_dt = grid_search.best_estimator_\n",
    "y_pred_tuned = best_dt.predict(X_test_raw)\n",
    "y_prob_tuned = best_dt.predict_proba(X_test_raw)[:, 1]\n",
    "\n",
    "print(\"\\n== Decision Tree (Tuned) Performance ==\\n\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_tuned):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_tuned):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_tuned):.3f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_tuned):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_prob_tuned):.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(best_dt, X_test_raw, y_test, cmap=\"Purples\")\n",
    "plt.title(\"Confusion Matrix - Decision Tree (Tuned)\")\n",
    "plt.grid(False)\n",
    "plt.savefig(\"Decision Tree tuned\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_tuned)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"Tuned Decision Tree (AUC = {roc_auc_score(y_test, y_prob_tuned):.2f})\", color='purple')\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Decision Tree (Tuned)\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison and Interpretation\n",
    "\n",
    "Compare Logistic Regression and Tuned Decision Tree models.  \n",
    "Visualize metrics (Accuracy, Precision, Recall, F1, ROC-AUC) to identify the stronger performer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Model Comparison and Interpretation (Final Visual Version)\n",
    "# ============================================\n",
    "\n",
    "\"\"\"\n",
    "Improved visual range to show all metrics (including lower Recall/F1 for Logistic Regression).\n",
    "Legend positioned at top-right.\n",
    "\"\"\"\n",
    "\n",
    "# Collect model metrics\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Decision Tree (Tuned)\"],\n",
    "    \"Accuracy\": [\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_tuned)\n",
    "    ],\n",
    "    \"Precision\": [\n",
    "        precision_score(y_test, y_pred_lr),\n",
    "        precision_score(y_test, y_pred_tuned)\n",
    "    ],\n",
    "    \"Recall\": [\n",
    "        recall_score(y_test, y_pred_lr),\n",
    "        recall_score(y_test, y_pred_tuned)\n",
    "    ],\n",
    "    \"F1 Score\": [\n",
    "        f1_score(y_test, y_pred_lr),\n",
    "        f1_score(y_test, y_pred_tuned)\n",
    "    ],\n",
    "    \"ROC-AUC\": [\n",
    "        roc_auc_score(y_test, y_prob_lr),\n",
    "        roc_auc_score(y_test, y_prob_tuned)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display table\n",
    "display(comparison_df.round(3))\n",
    "\n",
    "# Reshape for plotting\n",
    "comparison_df_melted = comparison_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    data=comparison_df_melted, \n",
    "    x=\"Metric\", y=\"Score\", hue=\"Model\", palette=\"coolwarm\", edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "plt.title(\"Model Comparison â€“ Logistic Regression vs Decision Tree (Tuned)\", fontsize=14, weight='bold')\n",
    "plt.ylim(0.2, 1.0)  # <-- Adjusted to display lower scores\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xlabel(\"Performance Metric\", fontsize=12)\n",
    "plt.grid(axis=\"y\", linestyle= \"--\", alpha=0.7)\n",
    "plt.legend(title=\"Model\", loc=\"upper right\", fontsize=10, title_fontsize=11, frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Log Regression vs Decision Tree(GridSearch)\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# --- Interpretation ---\n",
    "print(\"== Interpretation ==\")\n",
    "print(\"\"\"\n",
    "â€¢ Logistic Regression performs better on Precision and F1, indicating fewer false positives but weaker recall.\n",
    "â€¢ Decision Tree (Tuned) improves Recall and ROC-AUC, showing stronger sensitivity in identifying churners.\n",
    "â€¢ The trade-off is typical: Decision Tree captures more churners (better recall) but slightly sacrifices precision.\n",
    "â€¢ Overall, for business use, the Tuned Decision Tree offers better balance â€” ideal for proactive churn prevention.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Recommendations and Business Insights\n",
    "\n",
    "Translate model results into actionable business recommendations.  \n",
    "Identify top churn drivers and propose strategies for customer retention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Final Recommendations & Business Insights\n",
    "# ============================================\n",
    "\n",
    "\"\"\"\n",
    "This section translates model findings into plain-language insights for business decision-makers.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(\"\"\"\n",
    "# ðŸ“Š Final Recommendations & Business Insights\n",
    "\n",
    "### ðŸ” Key Findings\n",
    "\n",
    "1. **Churn Rate:** The dataset shows that approximately **14â€“15%** of customers have churned.  \n",
    "   This imbalance suggests that churn is a significant but not dominant issue.\n",
    "\n",
    "2. **Important Drivers of Churn (from Decision Tree Analysis):**\n",
    "   - **Customer Service Calls:** Frequent calls to support strongly correlate with churn risk.  \n",
    "   - **International Plan:** Customers with international plans are more likely to churn.  \n",
    "   - **Total Day Minutes / Charges:** Higher usage is often associated with increased churn risk, \n",
    "     possibly due to higher billing or dissatisfaction with plan costs.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¤– Model Comparison Summary\n",
    "\n",
    "| Metric | Logistic Regression | Decision Tree (Tuned) |\n",
    "|:-------|:-------------------:|:---------------------:|\n",
    "| Accuracy | ~0.94 | ~0.94 |\n",
    "| Precision | ~0.83 | ~0.83 |\n",
    "| Recall | ~0.28 | ~0.72 |\n",
    "| F1 Score | ~0.39 | ~0.77 |\n",
    "| ROC-AUC | ~0.80 | ~0.84 |\n",
    "\n",
    "- **Logistic Regression**: Better precision (fewer false churn predictions).  \n",
    "- **Decision Tree (Tuned)**: Higher recall and balanced F1 â€” captures more true churners.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§­ Interpretation\n",
    "\n",
    "- **Decision Tree (Tuned)** generalizes better and identifies churners more effectively (higher Recall).\n",
    "- **ROC-AUC of 0.84** confirms strong discriminatory power â€” much better than random guessing (0.5).\n",
    "- This means the model can reliably rank customers by churn likelihood.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Business Recommendations\n",
    "\n",
    "1. **Proactive Retention Campaigns:**  \n",
    "   Use the Decision Tree model to flag customers at high churn risk and target them with personalized offers (e.g., loyalty discounts, service upgrades).\n",
    "\n",
    "2. **Improve Customer Support Experience:**  \n",
    "   Customers who make frequent support calls are at high churn risk.  \n",
    "   Investigate call logs to identify common issues and improve resolution time.\n",
    "\n",
    "3. **Reevaluate International Plans:**  \n",
    "   Consider restructuring or re-pricing international plans â€” they are a key churn driver.\n",
    "\n",
    "4. **Monitor High-Usage Customers:**  \n",
    "   Implement predictive alerts when customer usage spikes unexpectedly (possible dissatisfaction or billing concerns).\n",
    "\n",
    "5. **Model Deployment & Monitoring:**  \n",
    "   - Deploy the tuned Decision Tree as a real-time scoring API or batch process.  \n",
    "   - Review performance quarterly to avoid model drift as customer behavior changes.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "1. **Feature Importance Visualization:** Create a plot of top features to communicate model insights visually.  \n",
    "2. **Model Optimization:** Experiment with ensemble methods (Random Forest, XGBoost) for future improvement.  \n",
    "3. **Business Integration:** Collaborate with marketing and customer service teams to act on model outputs.\n",
    "\n",
    "---\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Visualization\n",
    "\n",
    "Plot the top 10 most important features driving customer churn.  \n",
    "This improves interpretability and helps the business focus on key customer behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Feature Importance Visualization\n",
    "# ============================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Extract feature names from preprocessor\n",
    "num_features = numeric_cols\n",
    "cat_features = best_dt.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    "all_features = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# Extract feature importances from the tuned Decision Tree\n",
    "importances = best_dt.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "# Combine into a DataFrame\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": all_features,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Select top 10 most impactful features\n",
    "top_features = feature_importances.head(10)\n",
    "\n",
    "# Plot top 10 features\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=top_features, palette=\"coolwarm\")\n",
    "plt.title(\"Top 10 Drivers of Customer Churn (Decision Tree)\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"Relative Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save chart for PowerPoint\n",
    "\n",
    "plt.savefig(\"top 10 churn feautures\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "feature_chart_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
